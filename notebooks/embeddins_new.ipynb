{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62a46f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87ed8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Функция токенизации: перевод в нижний регистр и выделение слов (учитывая буквы кириллицы)\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # регулярное выражение ищет последовательности букв и цифр (\\w в Unicode)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text, flags=re.UNICODE)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2939e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Загрузка данных из папки dataset/ (подкаталоги: neg, neu, pos)\n",
    "data = []  # список словарей с ключами: 'filename', 'label', 'text'\n",
    "dataset_path = \"C:/Users/szlat/Hates-speech-detection/dataset\"  # имя папки с данными\n",
    "# Если требуется, можно задать соответствие меток, например:\n",
    "label_map = {\n",
    "    \"neg\": \"negative\",\n",
    "    \"neu\": \"neutral\",\n",
    "    \"pos\": \"positive\"\n",
    "}\n",
    "\n",
    "for folder in [\"neg\", \"neu\", \"pos\"]:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    # Обработка каждого файла с расширением .txt\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            data.append({\n",
    "                \"filename\": filename,\n",
    "                \"label\": label_map.get(folder, folder),\n",
    "                \"text\": text\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68911e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Токенизация текстов и построение словаря\n",
    "word_counts = {}  # подсчёт частот для каждого слова\n",
    "for item in data:\n",
    "    tokens = tokenize(item[\"text\"])\n",
    "    item[\"tokens\"] = tokens  # сохраняем список токенов для последующего использования\n",
    "    for token in tokens:\n",
    "        word_counts[token] = word_counts.get(token, 0) + 1\n",
    "\n",
    "# Можно при желании отсеять редкие слова (например, min_count=1 оставит все)\n",
    "min_count = 1\n",
    "vocab = {word: idx for idx, (word, count) in enumerate(word_counts.items()) if count >= min_count}\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Размер словаря: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e79086",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     16\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, filename)\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     18\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     19\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: filename,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: label_map\u001b[38;5;241m.\u001b[39mget(folder, folder),\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text\n\u001b[0;32m     23\u001b[0m     })\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Формирование обучающих примеров для модели CBOW\n",
    "# Для каждого слова в документе используем context window (например, по 2 слова слева и справа)\n",
    "training_samples = []  # список кортежей: ([индексы контекста], target_index)\n",
    "window_size = 2\n",
    "\n",
    "for item in data:\n",
    "    tokens = item[\"tokens\"]\n",
    "    # для каждого слова в предложении\n",
    "    for i in range(len(tokens)):\n",
    "        target_word = tokens[i]\n",
    "        # если слово не в словаре (хотя по построению все должны быть в vocab), пропускаем\n",
    "        if target_word not in vocab:\n",
    "            continue\n",
    "        target_idx = vocab[target_word]\n",
    "        context_indices = []\n",
    "        # определяем границы окна с учётом начала/конца документа\n",
    "        for j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n",
    "            if j == i:\n",
    "                continue  # пропускаем целевое слово\n",
    "            word = tokens[j]\n",
    "            if word in vocab:\n",
    "                context_indices.append(vocab[word])\n",
    "        if len(context_indices) > 0:\n",
    "            training_samples.append((context_indices, target_idx))\n",
    "\n",
    "print(f\"Количество обучающих примеров: {len(training_samples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Инициализация параметров для модели CBOW\n",
    "embedding_dim = 50  # размерность эмбеддингов (гиперпараметр)\n",
    "# Матрица эмбеддингов для входных слов: размер (vocab_size, embedding_dim)\n",
    "W1 = np.random.rand(vocab_size, embedding_dim) - 0.5\n",
    "# Матрица для выходного слоя: размер (embedding_dim, vocab_size)\n",
    "W2 = np.random.rand(embedding_dim, vocab_size) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2eb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Функция softmax\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6259b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Обучение модели CBOW (полный softmax; для небольшого словаря и датасета – вполне приемлемо)\n",
    "lr = 0.01     # скорость обучения\n",
    "epochs = 5    # число эпох\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    # перемешиваем обучающие примеры\n",
    "    np.random.shuffle(training_samples)\n",
    "    \n",
    "    for context_indices, target_idx in training_samples:\n",
    "        # Вычисляем усредненное представление контекста:\n",
    "        v_context = np.mean(W1[context_indices], axis=0)  # shape: (embedding_dim,)\n",
    "        \n",
    "        # Вычисляем \"сырые\" оценки (logits) для каждого слова словаря\n",
    "        z = np.dot(v_context, W2)  # shape: (vocab_size,)\n",
    "        y_pred = softmax(z)         # прогноз распределения вероятностей\n",
    "        \n",
    "        # Вычисление функции потерь (кросс-энтропия)\n",
    "        loss = -np.log(y_pred[target_idx] + 1e-9)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Вычисляем градиент ошибки:\n",
    "        # Начинаем с разности предсказанного распределения и истинного one-hot вектора\n",
    "        dz = y_pred.copy()\n",
    "        dz[target_idx] -= 1  # градиент по выходу\n",
    "        \n",
    "        # Градиенты для W2: outer product от вектора контекста и dz\n",
    "        dW2 = np.outer(v_context, dz)  # shape: (embedding_dim, vocab_size)\n",
    "        \n",
    "        # Градиент для усредненного вектора контекста:\n",
    "        dv = np.dot(W2, dz)  # shape: (embedding_dim,)\n",
    "        \n",
    "        # Так как v_context = average(W1[context_indices]),\n",
    "        # градиент для каждого слова из контекста равен dv / (число слов в контексте)\n",
    "        dv_each = dv / len(context_indices)\n",
    "        \n",
    "        # Обновляем веса для слов из контекста\n",
    "        for idx in context_indices:\n",
    "            W1[idx] -= lr * dv_each\n",
    "        \n",
    "        # Обновляем веса выходного слоя\n",
    "        W2 -= lr * dW2\n",
    "\n",
    "    avg_loss = total_loss / len(training_samples)\n",
    "    print(f\"Эпоха {epoch+1}/{epochs}, средняя потеря: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8157c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. После обучения матрица W1 содержит эмбеддинги для каждого слова.\n",
    "# Можно сохранить word embeddings в файл (например, word_embeddings.txt):\n",
    "with open(\"word_embeddings.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word, idx in vocab.items():\n",
    "        vector = \" \".join([f\"{val:.6f}\" for val in W1[idx]])\n",
    "        f.write(f\"{word} {vector}\\n\")\n",
    "print(\"Word embeddings сохранены в 'word_embeddings.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Вычисление эмбеддингов для отзывов (документ-эмбеддинги = среднее эмбеддингов слов в отзыве)\n",
    "# Результаты сохраняем в файл review_embeddings.csv с колонками: filename, label, embedding\n",
    "with open(\"review_embeddings.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"filename\", \"label\", \"embedding\"])  # header\n",
    "    for item in data:\n",
    "        tokens = item[\"tokens\"]\n",
    "        # Для каждого слова достаём его эмбеддинг (если слово есть в словаре)\n",
    "        vecs = [W1[vocab[token]] for token in tokens if token in vocab]\n",
    "        if len(vecs) > 0:\n",
    "            doc_embedding = np.mean(vecs, axis=0)\n",
    "        else:\n",
    "            doc_embedding = np.zeros(embedding_dim)\n",
    "        # Представляем вектор в виде строки (числа разделены пробелом)\n",
    "        embedding_str = \" \".join([f\"{val:.6f}\" for val in doc_embedding.tolist()])\n",
    "        writer.writerow([item[\"filename\"], item[\"label\"], embedding_str])\n",
    "print(\"Эмбеддинги отзывов сохранены в 'review_embeddings.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
