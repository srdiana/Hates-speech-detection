{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0E9dy0liZF6X",
    "outputId": "ba8bcae5-a735-4c0d-f713-4462be0795c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy3 in /opt/conda/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.1+cu118)\n",
      "Requirement already satisfied: umap-learn in /opt/conda/lib/python3.10/site-packages (0.5.7)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: dawg2-python>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pymorphy3) (0.9.0)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in /opt/conda/lib/python3.10/site-packages (from pymorphy3) (2.4.417150.4580142)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (0.58.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy3 nltk scikit-learn torch umap-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6T-Nl4r4aMqF",
    "outputId": "6aed0dae-657a-4b2d-9b6f-0c0ea6146862"
   },
   "outputs": [],
   "source": [
    "# !unzip 'archive (4).zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXXVUaRmbEoX",
    "outputId": "3ef3ecbf-71ee-475a-9fab-ad0dee29d726"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# from pymorphy3 import MorphAnalyzer\n",
    "# import re\n",
    "# import nltk\n",
    "\n",
    "# nltk.download('stopwords')  # Скачиваем стоп-слова для всех языков\n",
    "# nltk.download('punkt')      # Токенизатор (может потребоваться)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmfY-lj4bV8U",
    "outputId": "aed5c400-ce69-4067-b2b9-4672377f01f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('russian')[:10])  # Пример: ['и', 'в', 'во', 'не', 'что', ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kkdWPawgZL4y"
   },
   "outputs": [],
   "source": [
    "# morph = MorphAnalyzer()\n",
    "\n",
    "def load_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "# Загрузка данных\n",
    "pos_texts = load_texts_from_folder(\"dataset/pos\")\n",
    "neg_texts = load_texts_from_folder(\"dataset/neg\")\n",
    "neut_texts = load_texts_from_folder(\"dataset/neu\")\n",
    "\n",
    "# Объединение и маркировка\n",
    "texts = pos_texts + neg_texts + neut_texts\n",
    "labels = [1] * len(pos_texts) + [0] * len(neg_texts) + [2] * len(neut_texts)  # 1=pos, 0=neg, 2=neut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6fZv2OjazrA"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import concurrent.futures\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Инициализация объектов\n",
    "morph = MorphAnalyzer()\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Удаление пунктуации с использованием str.translate\n",
    "    text = text.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'))\n",
    "    text = text.lower()  # Приведение к нижнему регистру\n",
    "    words = text.split()\n",
    "\n",
    "    # Лемматизация с использованием множества для ускорения\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word not in stop_words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    \"\"\"Параллельная обработка всех текстов\"\"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        return list(executor.map(preprocess, texts))\n",
    "\n",
    "# Пример использования:\n",
    "processed_texts = preprocess_texts(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTt0CtVwcPEo"
   },
   "outputs": [],
   "source": [
    "# Сохранение processed_texts в файл (каждый текст на новой строке)\n",
    "with open('processed_texts.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in processed_texts:\n",
    "        f.write(text + '\\n')  # Добавляем перенос строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "h68mDYboybI-"
   },
   "outputs": [],
   "source": [
    "# 1. Загрузка предобработанных текстов из файла\n",
    "def load_processed_texts(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        processed_texts = [line.strip() for line in f.readlines()]\n",
    "    return processed_texts\n",
    "\n",
    "# Укажите путь к вашему файлу\n",
    "processed_texts = load_processed_texts('processed_texts (1).txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YM3vGFzqy4L8"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWV0QR7uy4Kz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnlsWBusUmiN",
    "outputId": "a8abca17-e93d-4bb8-a652-d61995624acd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность векторных представлений: (131669, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    token_pattern=r'(?u)\\b\\w+\\b',\n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "X = vectorizer.fit_transform(processed_texts).toarray()\n",
    "print(f\"Размерность векторных представлений: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bz_gEmvJU3AV"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=500):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train = torch.FloatTensor(X_train).to(device)\n",
    "X_test = torch.FloatTensor(X_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "model = Autoencoder(input_dim, embedding_dim=500).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.0020, Test Loss: 0.0646\n",
      "Epoch 2/10, Train Loss: 0.0020, Test Loss: 0.0645\n",
      "Epoch 3/10, Train Loss: 0.0019, Test Loss: 0.0554\n",
      "Epoch 4/10, Train Loss: 0.0017, Test Loss: 0.0535\n",
      "Epoch 5/10, Train Loss: 0.0017, Test Loss: 0.0527\n",
      "Epoch 6/10, Train Loss: 0.0016, Test Loss: 0.0523\n",
      "Epoch 7/10, Train Loss: 0.0016, Test Loss: 0.0520\n",
      "Epoch 8/10, Train Loss: 0.0016, Test Loss: 0.0517\n",
      "Epoch 9/10, Train Loss: 0.0016, Test Loss: 0.0514\n",
      "Epoch 10/10, Train Loss: 0.0016, Test Loss: 0.0511\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch = X_train[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = criterion(model(X_test), X_test).item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(X_train):.4f}, Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность эмбеддингов: (131669, 500)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings = model.encoder(torch.FloatTensor(X).to(device)).cpu().numpy()\n",
    "\n",
    "print(f\"Размерность эмбеддингов: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'autoencoder_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "OoA7gwLf9sAa",
    "outputId": "67a3d101-4e11-44f3-f38c-39877e0392e5"
   },
   "outputs": [],
   "source": [
    "with open('text_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'embeddings': embeddings,\n",
    "        'vocabulary': vectorizer.get_feature_names_out()\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UpzVUVv9U2pC",
    "outputId": "caabc810-d4a0-47d1-c365-4d20ad31c3ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved in final_embeddings.json with 131669 entries\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load the embeddings data\n",
    "with open('text_embeddings.pkl', 'rb') as f:\n",
    "    embeddings_data = pickle.load(f)\n",
    "\n",
    "# Create a list to store the output data in the desired format\n",
    "output_list = []\n",
    "\n",
    "# Get all filenames in order (pos first, then neg, then neut)\n",
    "filenames = []\n",
    "for folder, label in [('pos', 'positive'), ('neg', 'negative'), ('neu', 'neutral')]:\n",
    "    folder_path = f\"dataset/{folder}\"\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        filenames.append({\n",
    "            \"filename\": filename,\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "# Verify we have the same number of files as embeddings\n",
    "assert len(filenames) == len(embeddings_data['embeddings']), \"Mismatch between files and embeddings\"\n",
    "\n",
    "# Create the output list\n",
    "for i, (file_info, embedding) in enumerate(zip(filenames, embeddings_data['embeddings'])):\n",
    "    output_list.append({\n",
    "        \"filename\": file_info[\"filename\"],\n",
    "        \"label\": file_info[\"label\"],\n",
    "        \"embedding\": embedding.tolist()  # Convert numpy array to list\n",
    "    })\n",
    "\n",
    "\n",
    "with open('ae_embeddings.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data saved in final_embeddings.json with {len(output_list)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000023-0.txt</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-0.010793164372444153, 0.04824411869049072, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000023-1.txt</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.029604390263557434, 0.5547586679458618, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000023-10.txt</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-0.11132340878248215, 0.21457207202911377, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000023-2.txt</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.2803587317466736, -0.07406213879585266, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000023-4.txt</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-0.0559392049908638, 0.463557630777359, -0.20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename     label                                          embedding\n",
       "0   1000023-0.txt  positive  [-0.010793164372444153, 0.04824411869049072, 0...\n",
       "1   1000023-1.txt  positive  [0.029604390263557434, 0.5547586679458618, 0.3...\n",
       "2  1000023-10.txt  positive  [-0.11132340878248215, 0.21457207202911377, 0....\n",
       "3   1000023-2.txt  positive  [0.2803587317466736, -0.07406213879585266, -0....\n",
       "4   1000023-4.txt  positive  [-0.0559392049908638, 0.463557630777359, -0.20..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)  \n",
    "    return pd.DataFrame(data)\n",
    "df = load_data('ae_embeddings.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131669, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "X = np.array(df['embedding'].tolist())\n",
    "y = df['label_encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.51      0.57      2993\n",
      "           1       0.44      0.16      0.24      3643\n",
      "           2       0.77      0.94      0.85     13115\n",
      "\n",
      "    accuracy                           0.73     19751\n",
      "   macro avg       0.62      0.54      0.55     19751\n",
      "weighted avg       0.69      0.73      0.69     19751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=10)\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Logistic Regression Test Results:\")\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Logistic Regression Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.72      0.61      2993\n",
      "           1       0.33      0.47      0.39      3643\n",
      "           2       0.89      0.71      0.79     13115\n",
      "\n",
      "    accuracy                           0.66     19751\n",
      "   macro avg       0.58      0.63      0.59     19751\n",
      "weighted avg       0.73      0.66      0.69     19751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_balanced = LogisticRegression(max_iter=20, class_weight='balanced')\n",
    "lr_balanced.fit(X_train, y_train)\n",
    "print(\"Balanced Logistic Regression Test Results:\")\n",
    "print(classification_report(y_test, lr_balanced.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.23      0.31      2993\n",
      "           1       0.33      0.01      0.02      3643\n",
      "           2       0.70      0.96      0.81     13115\n",
      "\n",
      "    accuracy                           0.68     19751\n",
      "   macro avg       0.49      0.40      0.38     19751\n",
      "weighted avg       0.59      0.68      0.59     19751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=30,         \n",
    "    min_samples_split=40,\n",
    "    max_leaf_nodes=100      \n",
    ")\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"Decision Tree Test Results:\")\n",
    "print(classification_report(y_test, dt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Decision Tree Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.49      0.37      2993\n",
      "           1       0.23      0.43      0.30      3643\n",
      "           2       0.83      0.49      0.61     13115\n",
      "\n",
      "    accuracy                           0.48     19751\n",
      "   macro avg       0.45      0.47      0.43     19751\n",
      "weighted avg       0.64      0.48      0.52     19751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_balanced = DecisionTreeClassifier(random_state=42,\n",
    "    max_depth=500,         \n",
    "    min_samples_split=100,\n",
    "    max_leaf_nodes=750,  \n",
    "    class_weight='balanced')\n",
    "dt_balanced.fit(X_train, y_train)\n",
    "print(\"Balanced Decision Tree Test Results:\")\n",
    "print(classification_report(y_test, dt_balanced.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df['embedding'].tolist(), dtype=np.float32)  # Add dtype=np.float32\n",
    "y = df['label_encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "num_classes = len(le.classes_)\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1. / class_counts\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "model_balanced = NeuralNetwork(input_size, num_classes)\n",
    "optimizer = optim.Adam(model_balanced.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_balanced = model_balanced.to(device)\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model_balanced.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Val Loss: 501.4197 | Val Accuracy: 0.6713\n",
      "Epoch 2/10 | Val Loss: 504.1313 | Val Accuracy: 0.6861\n",
      "Epoch 3/10 | Val Loss: 497.3885 | Val Accuracy: 0.6699\n",
      "Epoch 4/10 | Val Loss: 495.4450 | Val Accuracy: 0.6686\n",
      "Epoch 5/10 | Val Loss: 494.1007 | Val Accuracy: 0.6749\n",
      "Epoch 6/10 | Val Loss: 493.9441 | Val Accuracy: 0.6699\n",
      "Epoch 7/10 | Val Loss: 493.9346 | Val Accuracy: 0.6708\n",
      "Epoch 8/10 | Val Loss: 495.8670 | Val Accuracy: 0.6610\n",
      "Epoch 9/10 | Val Loss: 494.5276 | Val Accuracy: 0.6550\n",
      "Epoch 10/10 | Val Loss: 496.8359 | Val Accuracy: 0.6388\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model_balanced.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_balanced(inputs)  # Теперь всё на GPU\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model_balanced.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Не забываем!\n",
    "            \n",
    "            outputs = model_balanced(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    val_acc = correct / len(val_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural Network Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60      2993\n",
      "           1       0.31      0.47      0.38      3643\n",
      "           2       0.90      0.67      0.77     13115\n",
      "\n",
      "    accuracy                           0.65     19751\n",
      "   macro avg       0.57      0.63      0.58     19751\n",
      "weighted avg       0.73      0.65      0.67     19751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_balanced.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_balanced(inputs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        \n",
    "        # Переносим тензоры на CPU перед преобразованием в numpy\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nNeural Network Test Results:\")\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Time: 8.42s\n",
      "Train Loss: 1.0330 | Val Loss: 0.8998\n",
      "Val Accuracy: 0.6082\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3834    0.7432    0.5058      2963\n",
      "           1     0.3074    0.3585    0.3310      3704\n",
      "           2     0.8756    0.6482    0.7450     13083\n",
      "\n",
      "    accuracy                         0.6082     19750\n",
      "   macro avg     0.5221    0.5833    0.5273     19750\n",
      "weighted avg     0.6952    0.6082    0.6314     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 2/20 | Time: 9.38s\n",
      "Train Loss: 0.9120 | Val Loss: 0.8533\n",
      "Val Accuracy: 0.6252\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3977    0.7830    0.5275      2963\n",
      "           1     0.3198    0.3539    0.3360      3704\n",
      "           2     0.8879    0.6663    0.7613     13083\n",
      "\n",
      "    accuracy                         0.6252     19750\n",
      "   macro avg     0.5352    0.6011    0.5416     19750\n",
      "weighted avg     0.7078    0.6252    0.6465     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 3/20 | Time: 8.57s\n",
      "Train Loss: 0.8774 | Val Loss: 0.8400\n",
      "Val Accuracy: 0.6257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4219    0.7587    0.5423      2963\n",
      "           1     0.3219    0.4241    0.3660      3704\n",
      "           2     0.8948    0.6526    0.7547     13083\n",
      "\n",
      "    accuracy                         0.6257     19750\n",
      "   macro avg     0.5462    0.6118    0.5543     19750\n",
      "weighted avg     0.7164    0.6257    0.6500     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 4/20 | Time: 7.75s\n",
      "Train Loss: 0.8628 | Val Loss: 0.8360\n",
      "Val Accuracy: 0.6229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4209    0.7681    0.5438      2963\n",
      "           1     0.3177    0.4190    0.3614      3704\n",
      "           2     0.8961    0.6478    0.7520     13083\n",
      "\n",
      "    accuracy                         0.6229     19750\n",
      "   macro avg     0.5449    0.6116    0.5524     19750\n",
      "weighted avg     0.7163    0.6229    0.6475     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 5/20 | Time: 7.52s\n",
      "Train Loss: 0.8543 | Val Loss: 0.8203\n",
      "Val Accuracy: 0.6579\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4712    0.7162    0.5684      2963\n",
      "           1     0.3390    0.4382    0.3822      3704\n",
      "           2     0.8842    0.7069    0.7857     13083\n",
      "\n",
      "    accuracy                         0.6579     19750\n",
      "   macro avg     0.5648    0.6204    0.5788     19750\n",
      "weighted avg     0.7200    0.6579    0.6774     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 6/20 | Time: 7.71s\n",
      "Train Loss: 0.8453 | Val Loss: 0.8163\n",
      "Val Accuracy: 0.6606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4768    0.7138    0.5717      2963\n",
      "           1     0.3405    0.4409    0.3842      3704\n",
      "           2     0.8840    0.7107    0.7879     13083\n",
      "\n",
      "    accuracy                         0.6606     19750\n",
      "   macro avg     0.5671    0.6218    0.5813     19750\n",
      "weighted avg     0.7210    0.6606    0.6798     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 7/20 | Time: 7.72s\n",
      "Train Loss: 0.8398 | Val Loss: 0.8139\n",
      "Val Accuracy: 0.6575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4773    0.7199    0.5740      2963\n",
      "           1     0.3377    0.4482    0.3852      3704\n",
      "           2     0.8868    0.7027    0.7841     13083\n",
      "\n",
      "    accuracy                         0.6575     19750\n",
      "   macro avg     0.5673    0.6236    0.5811     19750\n",
      "weighted avg     0.7224    0.6575    0.6778     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 8/20 | Time: 7.71s\n",
      "Train Loss: 0.8324 | Val Loss: 0.8112\n",
      "Val Accuracy: 0.6633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4810    0.7175    0.5759      2963\n",
      "           1     0.3407    0.4355    0.3823      3704\n",
      "           2     0.8836    0.7156    0.7908     13083\n",
      "\n",
      "    accuracy                         0.6633     19750\n",
      "   macro avg     0.5684    0.6229    0.5830     19750\n",
      "weighted avg     0.7214    0.6633    0.6819     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 9/20 | Time: 8.16s\n",
      "Train Loss: 0.8290 | Val Loss: 0.8082\n",
      "Val Accuracy: 0.6666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4987    0.6932    0.5801      2963\n",
      "           1     0.3394    0.4509    0.3873      3704\n",
      "           2     0.8815    0.7217    0.7936     13083\n",
      "\n",
      "    accuracy                         0.6666     19750\n",
      "   macro avg     0.5732    0.6219    0.5870     19750\n",
      "weighted avg     0.7224    0.6666    0.6854     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 10/20 | Time: 7.90s\n",
      "Train Loss: 0.8239 | Val Loss: 0.8062\n",
      "Val Accuracy: 0.6671\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4954    0.7027    0.5811      2963\n",
      "           1     0.3390    0.4420    0.3837      3704\n",
      "           2     0.8823    0.7228    0.7946     13083\n",
      "\n",
      "    accuracy                         0.6671     19750\n",
      "   macro avg     0.5722    0.6225    0.5865     19750\n",
      "weighted avg     0.7223    0.6671    0.6855     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 11/20 | Time: 7.49s\n",
      "Train Loss: 0.8180 | Val Loss: 0.8043\n",
      "Val Accuracy: 0.6714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5044    0.6915    0.5833      2963\n",
      "           1     0.3411    0.4430    0.3854      3704\n",
      "           2     0.8798    0.7315    0.7988     13083\n",
      "\n",
      "    accuracy                         0.6714     19750\n",
      "   macro avg     0.5751    0.6220    0.5892     19750\n",
      "weighted avg     0.7225    0.6714    0.6890     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 12/20 | Time: 8.12s\n",
      "Train Loss: 0.8177 | Val Loss: 0.8036\n",
      "Val Accuracy: 0.6700\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5358    0.6493    0.5871      2963\n",
      "           1     0.3370    0.4906    0.3995      3704\n",
      "           2     0.8815    0.7254    0.7959     13083\n",
      "\n",
      "    accuracy                         0.6700     19750\n",
      "   macro avg     0.5848    0.6218    0.5942     19750\n",
      "weighted avg     0.7275    0.6700    0.6902     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 13/20 | Time: 7.44s\n",
      "Train Loss: 0.8161 | Val Loss: 0.8020\n",
      "Val Accuracy: 0.6638\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5085    0.6861    0.5841      2963\n",
      "           1     0.3345    0.4714    0.3913      3704\n",
      "           2     0.8860    0.7133    0.7903     13083\n",
      "\n",
      "    accuracy                         0.6638     19750\n",
      "   macro avg     0.5763    0.6236    0.5886     19750\n",
      "weighted avg     0.7259    0.6638    0.6846     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 14/20 | Time: 7.79s\n",
      "Train Loss: 0.8120 | Val Loss: 0.8012\n",
      "Val Accuracy: 0.6698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5305    0.6554    0.5864      2963\n",
      "           1     0.3374    0.4852    0.3980      3704\n",
      "           2     0.8816    0.7253    0.7959     13083\n",
      "\n",
      "    accuracy                         0.6698     19750\n",
      "   macro avg     0.5832    0.6220    0.5934     19750\n",
      "weighted avg     0.7269    0.6698    0.6898     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 15/20 | Time: 7.59s\n",
      "Train Loss: 0.8098 | Val Loss: 0.7997\n",
      "Val Accuracy: 0.6649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5097    0.6888    0.5859      2963\n",
      "           1     0.3361    0.4719    0.3926      3704\n",
      "           2     0.8860    0.7141    0.7908     13083\n",
      "\n",
      "    accuracy                         0.6649     19750\n",
      "   macro avg     0.5773    0.6250    0.5898     19750\n",
      "weighted avg     0.7264    0.6649    0.6854     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 16/20 | Time: 9.23s\n",
      "Train Loss: 0.8084 | Val Loss: 0.7993\n",
      "Val Accuracy: 0.6620\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5186    0.6730    0.5858      2963\n",
      "           1     0.3328    0.4906    0.3966      3704\n",
      "           2     0.8868    0.7081    0.7875     13083\n",
      "\n",
      "    accuracy                         0.6620     19750\n",
      "   macro avg     0.5794    0.6239    0.5899     19750\n",
      "weighted avg     0.7277    0.6620    0.6839     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 17/20 | Time: 8.87s\n",
      "Train Loss: 0.8062 | Val Loss: 0.7979\n",
      "Val Accuracy: 0.6739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5167    0.6821    0.5880      2963\n",
      "           1     0.3435    0.4611    0.3937      3704\n",
      "           2     0.8816    0.7322    0.8000     13083\n",
      "\n",
      "    accuracy                         0.6739     19750\n",
      "   macro avg     0.5806    0.6252    0.5939     19750\n",
      "weighted avg     0.7260    0.6739    0.6920     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 18/20 | Time: 7.70s\n",
      "Train Loss: 0.8065 | Val Loss: 0.7974\n",
      "Val Accuracy: 0.6658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5244    0.6716    0.5889      2963\n",
      "           1     0.3355    0.4903    0.3984      3704\n",
      "           2     0.8864    0.7142    0.7910     13083\n",
      "\n",
      "    accuracy                         0.6658     19750\n",
      "   macro avg     0.5821    0.6254    0.5928     19750\n",
      "weighted avg     0.7287    0.6658    0.6871     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 19/20 | Time: 7.54s\n",
      "Train Loss: 0.8026 | Val Loss: 0.7981\n",
      "Val Accuracy: 0.6815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5264    0.6774    0.5924      2963\n",
      "           1     0.3483    0.4463    0.3912      3704\n",
      "           2     0.8756    0.7490    0.8074     13083\n",
      "\n",
      "    accuracy                         0.6815     19750\n",
      "   macro avg     0.5834    0.6242    0.5970     19750\n",
      "weighted avg     0.7243    0.6815    0.6971     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 20/20 | Time: 7.59s\n",
      "Train Loss: 0.8019 | Val Loss: 0.7976\n",
      "Val Accuracy: 0.6818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5328    0.6615    0.5902      2963\n",
      "           1     0.3483    0.4565    0.3951      3704\n",
      "           2     0.8750    0.7501    0.8078     13083\n",
      "\n",
      "    accuracy                         0.6818     19750\n",
      "   macro avg     0.5854    0.6227    0.5977     19750\n",
      "weighted avg     0.7249    0.6818    0.6977     19750\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Final Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5431    0.6926    0.6088      2993\n",
      "           1     0.3338    0.4864    0.3959      3643\n",
      "           2     0.8889    0.7202    0.7957     13115\n",
      "\n",
      "    accuracy                         0.6729     19751\n",
      "   macro avg     0.5886    0.6331    0.6001     19751\n",
      "weighted avg     0.7341    0.6729    0.6936     19751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size // 2,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_prob if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_normal_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def initialize_model(X_train, y_train, num_classes):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_config = {\n",
    "        'input_size': X_train.shape[1],\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 2,\n",
    "        'output_size': num_classes,\n",
    "        'dropout_prob': 0.4\n",
    "    }\n",
    "    \n",
    "    model = BidirectionalLSTM(**model_config).to(device)\n",
    "    \n",
    "    training_config = {\n",
    "        'learning_rate': 0.00001,\n",
    "        'weight_decay': 1e-4,\n",
    "        'clip_value': 0.5\n",
    "    }\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=training_config['learning_rate'],\n",
    "        weight_decay=training_config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "    \n",
    "    return device, model, optimizer, criterion, scheduler\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, \n",
    "                scheduler=None, epochs=20, clip_value=0.5, patience=3):\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = correct / len(val_loader.dataset)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1}/{epochs} | Time: {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "        print(f'Val Accuracy: {val_acc:.4f}')\n",
    "        print(classification_report(all_labels, all_preds, digits=4))\n",
    "        print('-' * 60)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    print(\"\\nFinal Test Results:\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    device, model, optimizer, criterion, scheduler = initialize_model(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        num_classes=len(np.unique(y_train))\n",
    "    )\n",
    "    \n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        scheduler=scheduler,\n",
    "        epochs=20,\n",
    "        clip_value=0.5,\n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    test_preds, test_labels = evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
